{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99c9c18-f040-4e78-9d49-0c697895a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from sklearn.covariance import LedoitWolf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b7ac24-055a-4194-9740-8616ac029d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS = [\n",
    "    # Large-cap tech & communication\n",
    "    \"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"TSLA\",\"META\",\"NVDA\",\"NFLX\",\"ADBE\",\"CRM\",\n",
    "\n",
    "    # Semiconductors\n",
    "    \"INTC\",\"AMD\",\"QCOM\",\"AVGO\",\"TXN\",\"MU\",\"AMAT\",\"LRCX\",\"NXPI\",\"KLAC\",\n",
    "\n",
    "    # Banks & financials\n",
    "    \"JPM\",\"BAC\",\"WFC\",\"C\",\"GS\",\"MS\",\"V\",\"MA\",\"PYPL\",\"AXP\",\n",
    "\n",
    "    # Healthcare & pharma\n",
    "    \"PFE\",\"JNJ\",\"MRK\",\"UNH\",\"ABBV\",\"LLY\",\"BMY\",\"GILD\",\"AMGN\",\"REGN\",\n",
    "\n",
    "    # Energy\n",
    "    \"XOM\",\"CVX\",\"BP\",\"SHEL\",\"TTE\",\"COP\",\"SLB\",\"HAL\",\"MPC\",\"PSX\",\n",
    "\n",
    "    # Consumer staples & discretionary\n",
    "    \"PG\",\"KO\",\"PEP\",\"WMT\",\"COST\",\"HD\",\"MCD\",\"NKE\",\"TGT\",\"SBUX\",\n",
    "\n",
    "    # Industrials\n",
    "    \"BA\",\"CAT\",\"DE\",\"HON\",\"GE\",\"UPS\",\"MMM\",\"LMT\",\"RTX\",\"FDX\",\n",
    "\n",
    "    # Utilities\n",
    "    \"NEE\",\"DUK\",\"SO\",\"AEP\",\"EXC\",\"SRE\",\"WEC\",\"PEG\",\"ED\",\"XEL\",\n",
    "\n",
    "    # Telecom\n",
    "    \"VZ\",\"T\",\"TMUS\",\"CMCSA\",\"CHTR\",\"DIS\",\"FOX\",\"FOXA\",\"ROKU\",\n",
    "\n",
    "    # More large-caps for diversity\n",
    "    \"ORCL\",\"IBM\",\"SAP\",\"CSCO\",\"MMM\",\"BKNG\",\"SPGI\",\"BLK\",\"MO\",\"PM\",\n",
    "    \"ADSK\",\"WDAY\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21ee7ff-b78f-474b-adfe-020f6f42ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "END = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "RISK_FREE_RATE = 0.035\n",
    "ANNUAL_TRADING_DAYS = 252\n",
    "OUTPUT_DIR = \"optiverse_outputs\"\n",
    "\n",
    "L2_REG = 1e-2\n",
    "EPS_JITTER = 1e-5\n",
    "ALLOW_SHORT = False\n",
    "N_POINTS = 50\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "LOOKBACK_YEARS = [3, 5, 10]\n",
    "MIN_DATA_FRACTION = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a8166-5656-4edd-a1d8-cf66c2c61a9c",
   "metadata": {},
   "source": [
    "## DOWNLOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aaa978b-4205-4e46-8d14-1d790a51bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to download the prices from yfinance for the following stocks in tickers from the dates containing from start to end.\n",
    "#and placing them properly in a dataframe.\n",
    "def download_prices(tickers, start, end=END):\n",
    "    raw = yf.download(tickers, start=start, end=end, progress=False, auto_adjust=False)\n",
    "    if raw.empty:\n",
    "        raise RuntimeError(\"yfinance returned no data for the given tickers / date range.\")\n",
    "    if isinstance(raw.columns, pd.MultiIndex):\n",
    "        if \"Adj Close\" in raw.columns.get_level_values(0):\n",
    "            df = raw[\"Adj Close\"]\n",
    "        elif \"Close\" in raw.columns.get_level_values(0):\n",
    "            df = raw[\"Close\"]\n",
    "        else:\n",
    "            df = raw.xs(raw.columns.levels[0][-1], axis=1, level=0, drop_level=True)\n",
    "    else:\n",
    "        if \"Adj Close\" in raw.columns:\n",
    "            df = raw[\"Adj Close\"]\n",
    "        elif \"Close\" in raw.columns:\n",
    "            df = raw[\"Close\"]\n",
    "        else:\n",
    "            df = raw\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    df = df.dropna(how=\"all\")\n",
    "    if list(df.columns) != list(tickers) and len(df.columns) == len(tickers):\n",
    "        try:\n",
    "            df.columns = tickers\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "177f3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker dropping explanation:\n",
    "# We drop any ticker that does NOT have at least MIN_DATA_FRACTION (90%) of the required\n",
    "# trading days for the window. Missing data would otherwise break covariance estimation\n",
    "# and produce unstable or inaccurate results.\n",
    "def filter_tickers_by_history(prices, required_days):\n",
    "    avail = prices.notna().sum()\n",
    "    kept = list(avail[avail >= required_days].index)\n",
    "    dropped = list(avail[avail < required_days].index)\n",
    "    return kept, dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ca89f-2f94-4c41-982b-69a6a5199068",
   "metadata": {},
   "source": [
    "## SOLVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fa3dde-58af-447f-8c67-8e77363eefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in solve markovwitx funtion we use solvers like OSQP,ECOS,SCS to solve the optimization problem. \n",
    "# these solvers essentially solve the KKT and largarian conditions\n",
    "#and we return the optimal weights allocations . \n",
    "\n",
    "def solve_markowitz(mu, Sigma, R_target, allow_short=False, l2_reg=0.0):\n",
    "    n = Sigma.shape[0]\n",
    "    w = cp.Variable((n, 1))\n",
    "    objective = cp.quad_form(w, Sigma)\n",
    "    if l2_reg and l2_reg > 0:\n",
    "        objective = objective + l2_reg * cp.sum_squares(w)\n",
    "    constraints = [cp.sum(w) == 1, mu.T @ w >= R_target]\n",
    "    if not allow_short:\n",
    "        constraints.append(w >= 0)\n",
    "    prob = cp.Problem(cp.Minimize(objective), constraints)\n",
    "\n",
    "    # Trying OSQP with stricter tolerances first\n",
    "    try:\n",
    "        prob.solve(solver=cp.OSQP, verbose=False, max_iter=200000, eps_abs=1e-6, eps_rel=1e-6)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback to ECOS\n",
    "    if prob.status not in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "        try:\n",
    "            prob.solve(solver=cp.ECOS, verbose=False, max_iters=100000)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # last fallback to SCS\n",
    "    if prob.status not in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "        try:\n",
    "            prob.solve(solver=cp.SCS, verbose=False, max_iters=25000)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if prob.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "        w_opt = np.array(w.value).flatten()\n",
    "    else:\n",
    "        w_opt = np.full(n, np.nan)\n",
    "    return w_opt, prob.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea13cc0b-56d7-4929-b334-ada8baa69e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is compute_returns, we know using markowitz , the returns are computed from mean, and risk from covaraince\n",
    "# and we use Ledoit-wolf shrinkage to make the covarience matrix to make it close to a target matrix, like the identity matrix, to create a more stable and well-conditioned estimate\n",
    "def compute_returns(prices, use_shrinkage=True):\n",
    "    rets = prices.pct_change().dropna()\n",
    "    mean_daily = rets.mean()\n",
    "    cov_daily = rets.cov()\n",
    "    mu = mean_daily * ANNUAL_TRADING_DAYS\n",
    "    Sigma = cov_daily * ANNUAL_TRADING_DAYS\n",
    "\n",
    "    if use_shrinkage:\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(rets.values)\n",
    "            Sigma = pd.DataFrame(lw.covariance_, index=prices.columns, columns=prices.columns)\n",
    "            Sigma = Sigma * ANNUAL_TRADING_DAYS\n",
    "        except Exception as e:\n",
    "            print(\"Ledoit-Wolf shrinkage failed, using sample covariance. Error:\", e)\n",
    "\n",
    "    # Adding jitter to diagonal to avoid singularity\n",
    "    Sigma = Sigma + EPS_JITTER * np.eye(Sigma.shape[0])\n",
    "\n",
    "    return rets, mu.values.reshape(-1, 1), Sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e60784-90d2-4e2b-8c46-35c2e11fe9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this function, we apply the logic, for a particular target returns and we call the\n",
    "#  solve_markkoizw funtion and apply l2 regularization and return the final optimaization weights\n",
    "\n",
    "def compute_efficient_frontier(mu, Sigma, n_points=50, allow_short=False, l2_reg=0.0):\n",
    "    mu_vec = mu.flatten()\n",
    "    mu_min = mu_vec.min()\n",
    "    mu_max = mu_vec.max()\n",
    "    targets = np.linspace(mu_min * 0.9, mu_max * 1.1, n_points)\n",
    "    weights = []\n",
    "    risks = []\n",
    "    rets = []\n",
    "    statuses = []\n",
    "    for R in targets:\n",
    "        w_opt, status = solve_markowitz(mu, Sigma, R, allow_short=allow_short, l2_reg=l2_reg)\n",
    "        statuses.append(status)\n",
    "        if np.any(np.isnan(w_opt)):\n",
    "            weights.append(np.full_like(mu_vec, np.nan))\n",
    "            risks.append(np.nan)\n",
    "            rets.append(np.nan)\n",
    "        else:\n",
    "            weights.append(w_opt)\n",
    "            port_ret = float(mu_vec @ w_opt)\n",
    "            port_var = float(w_opt.T @ Sigma @ w_opt)\n",
    "            risks.append(math.sqrt(port_var))\n",
    "            rets.append(port_ret)\n",
    "    df = pd.DataFrame({\"target\": targets, \"ret\": rets, \"risk\": risks})\n",
    "    weights_df = pd.DataFrame(weights, columns=[f for f in range(len(mu_vec))])\n",
    "    return df, weights_df, statuses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2668de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute sharpe _Ratio which measures how much each asset improves or wrosens.\n",
    "#It combines return AND risk of the asset.\n",
    "#High Sharpe contribution,the asset is improving the portfolio’s risk-adjusted return.\n",
    "#Low or negative Sharpe contribution, the asset may be hurting your Sharpe ratio.\n",
    "def sharpe_ratio(ret, risk, risk_free=RISK_FREE_RATE):\n",
    "    return (ret - risk_free) / risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d875b8f-0c28-40b4-a121-dedf94ca128c",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af83f65-67b9-49b3-ab2a-7c527ef20a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a return vs risk graph of the investments\n",
    "\n",
    "def plot_efficient_frontier(df, label=None, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(df[\"risk\"], df[\"ret\"], marker=\"o\", linewidth=1, label=label)\n",
    "    ax.set_xlabel(\"Annualized Volatility (Std Dev)\")\n",
    "    ax.set_ylabel(\"Annualized Return\")\n",
    "    ax.grid(True)\n",
    "    if label:\n",
    "        ax.legend()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b14423-fc9d-446d-97de-ad701a54fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after finding the optimal allocation of weights we visually represent it in the form of a bar graph\n",
    "\n",
    "def plot_nonzero_weights(weights_series, title=\"Nonzero Portfolio Weights\", output_path=None, top_n=None):\n",
    "    # Drop zero or almost zero weights\n",
    "    ws = weights_series[weights_series.abs() > 1e-6].copy()\n",
    "    if ws.empty:\n",
    "        print(\"No non-zero weights to plot.\")\n",
    "        return\n",
    "    if top_n is not None and len(ws) > top_n:\n",
    "        ws = ws.reindex(ws.abs().sort_values(ascending=False).index[:top_n])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ws.sort_values(ascending=True).plot(kind='barh')\n",
    "    plt.xlabel('Weight')\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='x')\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e9afb",
   "metadata": {},
   "source": [
    "## ANALYSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00295a-9e07-4e28-b728-9a9b8a84f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each lookback window (e.g. 3y, 5y, 10y):Downloads price data.\n",
    "#Computes returns, mean vector mu, and covariance matrix Sigma with shrinkage.\n",
    "#Builds the efficient frontier (many portfolios with different target returns. \n",
    "# then we plot all efficient frontiers on a single chart for comparison.\n",
    "def compare_frontiers_for_windows(tickers, lookback_years=LOOKBACK_YEARS, end=END, n_points=N_POINTS, l2_reg=L2_REG):\n",
    "    results = {}\n",
    "    end_dt = pd.to_datetime(end)\n",
    "    for yrs in lookback_years:\n",
    "        start_dt = end_dt - pd.DateOffset(years=yrs)\n",
    "        start_str = start_dt.strftime('%Y-%m-%d')\n",
    "        print(f\"--- Window: last {yrs} years ({start_str} to {end}) ---\")\n",
    "        try:\n",
    "            prices = download_prices(tickers, start=start_str, end=end)\n",
    "        except RuntimeError as e:\n",
    "            print(\"Data download failed for window\", yrs, \"-> skipping\")\n",
    "            continue\n",
    "        required_days = int(ANNUAL_TRADING_DAYS * yrs * MIN_DATA_FRACTION)\n",
    "        kept, dropped = filter_tickers_by_history(prices, required_days)\n",
    "        print(f\"Tickers kept: {len(kept)}, dropped: {len(dropped)}\")\n",
    "        if dropped:\n",
    "            print(\"Dropped tickers:\", dropped)\n",
    "        if len(kept) < 3:\n",
    "            print(\"Not enough tickers after filtering — skipping this window.\")\n",
    "            continue\n",
    "        prices_kept = prices[kept].dropna(how='all')\n",
    "        rets, mu, Sigma = compute_returns(prices_kept, use_shrinkage=True)\n",
    "\n",
    "        # diagnostics: condition number\n",
    "        try:\n",
    "            cond = np.linalg.cond(Sigma)\n",
    "            print(\"Sigma condition number:\", cond)\n",
    "        except Exception:\n",
    "            print(\"Could not compute condition number for Sigma\")\n",
    "\n",
    "        df, weights_df, statuses = compute_efficient_frontier(mu, Sigma, n_points=n_points, allow_short=ALLOW_SHORT, l2_reg=l2_reg)\n",
    "        valid = df['risk'].notna()\n",
    "        df = df[valid].reset_index(drop=True)\n",
    "        weights_df = weights_df[valid].reset_index(drop=True)\n",
    "        results[yrs] = {\n",
    "            'start': start_str,\n",
    "            'end': end,\n",
    "            'prices': prices_kept,\n",
    "            'mu': mu,\n",
    "            'Sigma': Sigma,\n",
    "            'ef': df,\n",
    "            'weights': weights_df,\n",
    "            'kept': kept,\n",
    "            'dropped': dropped,\n",
    "            'statuses': statuses\n",
    "        }\n",
    "        df.to_csv(os.path.join(OUTPUT_DIR, f'efficient_frontier_{yrs}y.csv'), index=False)\n",
    "        weights_df.to_csv(os.path.join(OUTPUT_DIR, f'weights_{yrs}y.csv'), index=False)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = plt.gca()\n",
    "    for yrs, r in results.items():\n",
    "        plot_efficient_frontier(r['ef'], label=f'{yrs}y', ax=ax)\n",
    "    plt.title('Efficient Frontier Comparison (3y / 5y / 10y)')\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'frontier_comparison.png'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d99f9-e820-4fbb-88bd-d9d6da97f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_window(tickers, start, end=END, n_points=N_POINTS, allow_short=ALLOW_SHORT, l2_reg=L2_REG):\n",
    "    print(\"Downloading prices for:\", tickers)\n",
    "    prices = download_prices(tickers, start=start, end=end)\n",
    "    print(\"Prices shape:\", prices.shape)\n",
    "\n",
    "    rets, mu, Sigma = compute_returns(prices, use_shrinkage=True)\n",
    "\n",
    "    print(\"Estimated annualized expected returns (mu):\")\n",
    "    mu_ser = pd.Series(mu.flatten(), index=prices.columns)\n",
    "    print(mu_ser)\n",
    "\n",
    "    print(\"Estimated annualized covariance matrix diagonal (annual variance):\")\n",
    "    print(pd.Series(np.diag(Sigma), index=prices.columns))\n",
    "\n",
    "    print(\"Any NaNs in returns? per-ticker:\")\n",
    "    print(rets.isna().sum())\n",
    "\n",
    "    try:\n",
    "        cond = np.linalg.cond(Sigma)\n",
    "        print(\"Sigma condition number:\", cond)\n",
    "    except Exception:\n",
    "        print(\"Could not compute condition number for Sigma\")\n",
    "\n",
    "    ef_df, weights_df, statuses = compute_efficient_frontier(mu, Sigma, n_points=n_points, allow_short=allow_short, l2_reg=l2_reg)\n",
    "\n",
    "    print(\"Solver statuses sample:\", statuses[:min(10, len(statuses))])\n",
    "    valid = ef_df[\"risk\"].notna()\n",
    "    ef_df = ef_df[valid].reset_index(drop=True)\n",
    "    weights_df = weights_df[valid].reset_index(drop=True)\n",
    "\n",
    "    print(\"Number of valid frontier points:\", len(ef_df))\n",
    "    nonzero_counts = (weights_df.abs() > 1e-6).sum(axis=1)\n",
    "    print(\"Nonzero asset counts per portfolio (median, min, max):\", nonzero_counts.median(), nonzero_counts.min(), nonzero_counts.max())\n",
    "\n",
    "    ef_df[\"sharpe\"] = ef_df.apply(lambda r: sharpe_ratio(r[\"ret\"], r[\"risk\"]), axis=1)\n",
    "    best_idx = ef_df[\"sharpe\"].idxmax()\n",
    "    best_weights = weights_df.loc[best_idx].values\n",
    "    best_ret = ef_df.loc[best_idx, \"ret\"]\n",
    "    best_risk = ef_df.loc[best_idx, \"risk\"]\n",
    "    best_sharpe = ef_df.loc[best_idx, \"sharpe\"]\n",
    "\n",
    "    print(f\"Best Sharpe portfolio (index {best_idx}): Return={best_ret:.4f}, Risk={best_risk:.4f}, Sharpe={best_sharpe:.4f}\")\n",
    "    weights_series = pd.Series(best_weights, index=prices.columns)\n",
    "    mu_ser_full = pd.Series(mu.flatten(), index=prices.columns)\n",
    "    sigma_diag = pd.Series(np.diag(Sigma), index=prices.columns)\n",
    "\n",
    "    print(\"Weights (best Sharpe):\")\n",
    "    print(weights_series)\n",
    "\n",
    "    ef_csv = os.path.join(OUTPUT_DIR, \"efficient_frontier_single.csv\")\n",
    "    ef_df.to_csv(ef_csv, index=False)\n",
    "    weights_csv = os.path.join(OUTPUT_DIR, \"best_weights_single.csv\")\n",
    "    weights_series.to_csv(weights_csv, header=[\"weight\"] )\n",
    "\n",
    "    plot_efficient_frontier(ef_df, label='efficient frontier')\n",
    "    # Plot only the non-zero weights for the best Sharpe portfolio (makes it clear what to invest in)\n",
    "    try:\n",
    "        plot_nonzero_weights(weights_series, title='Nonzero Weights — Best Sharpe Portfolio', output_path=os.path.join(OUTPUT_DIR, 'nonzero_weights_best_sharpe.png'))\n",
    "    except Exception as e:\n",
    "        print('Could not plot nonzero weights:', e)\n",
    "\n",
    "    print(\"================== PORTFOLIO REPORT ==================\")\n",
    "    print(f\"Data period: {start} to {end}\")\n",
    "    print(f\"Number of assets: {len(prices.columns)}\")\n",
    "    print(f\"Best Sharpe Ratio Portfolio:\")\n",
    "    print(f\"  Expected Return: {best_ret:.4f}\")\n",
    "    print(f\"  Risk (Volatility): {best_risk:.4f}\")\n",
    "    print(f\"  Sharpe Ratio: {best_sharpe:.4f}\")\n",
    "    print(\"  Allocation:\")\n",
    "    for t, w in weights_series.items():\n",
    "        print(f\"    {t}: {w:.4f}\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(\"This portfolio provides the highest return per unit of risk \")\n",
    "    print(\"among all efficient frontier portfolios. The weights indicate \")\n",
    "    print(\"which assets dominate the optimal risk-adjusted performance.\")\n",
    "    print(\"Plots and CSV outputs have been saved for further analysis.\")\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    print(\"Outputs saved to\", OUTPUT_DIR)\n",
    "\n",
    "    # --- Human-readable interpretation of holdings with weight > 0 ---\n",
    "    nonzero = weights_series[weights_series.abs() > 1e-6].sort_values(ascending=False)\n",
    "    print(\"=== INTERPRETATION OF HOLDINGS (weights > 0) ===\")\n",
    "    total_portfolio_var = float(weights_series.values.T @ Sigma @ weights_series.values)\n",
    "    # compute marginal and percent variance contributions\n",
    "    try:\n",
    "        sigma_w = Sigma.values @ weights_series.values\n",
    "    except Exception:\n",
    "        sigma_w = Sigma @ weights_series.values\n",
    "    var_contrib = (weights_series.values * sigma_w)\n",
    "    if total_portfolio_var > 0:\n",
    "        pct_var_contrib = 100.0 * var_contrib / total_portfolio_var\n",
    "    else:\n",
    "        pct_var_contrib = np.zeros_like(var_contrib)\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    for t, w in nonzero.items():\n",
    "        mu_i = float(mu_ser_full.get(t, np.nan))\n",
    "        var_i = float(sigma_diag.get(t, np.nan))\n",
    "        vol_i = math.sqrt(var_i) if not np.isnan(var_i) else np.nan\n",
    "    \n",
    "            # variance contribution\n",
    "        if t in prices.columns:\n",
    "            contrib_pct = pct_var_contrib[prices.columns.get_loc(t)]\n",
    "        else:\n",
    "            contrib_pct = np.nan\n",
    "    \n",
    "            # tags\n",
    "        if w >= 0.2:\n",
    "            tag = 'major allocation'\n",
    "        elif w >= 0.05:\n",
    "            tag = 'meaningful allocation'\n",
    "        else:\n",
    "            tag = 'small allocation'\n",
    "    \n",
    "            # store for dataframe\n",
    "        rows.append([t, w, mu_i, vol_i, contrib_pct, tag])\n",
    "    \n",
    "    # Build dataframe\n",
    "    nonzero_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"Ticker\",\n",
    "            \"Weight\",\n",
    "            \"ExpectedReturn\",\n",
    "            \"Volatility\",\n",
    "            \"VarianceContribution\",\n",
    "            \"AllocationTag\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Sharpe contribution\n",
    "    nonzero_df[\"SharpeContribution\"] = (\n",
    "        (nonzero_df[\"ExpectedReturn\"] - RISK_FREE_RATE) /\n",
    "        nonzero_df[\"Volatility\"]\n",
    "    ) * nonzero_df[\"Weight\"]\n",
    "    \n",
    "    # Sort by weight descending\n",
    "    nonzero_df = nonzero_df.sort_values(\"Weight\", ascending=False)\n",
    "    \n",
    "    print(\"\\n=== FINAL EXTENDED SUMMARY: Non-zero Weighted Stocks Sorted by Weight ===\")\n",
    "    for _, row in nonzero_df.iterrows():\n",
    "        print(\n",
    "            f\"{row['Ticker']}: weight={row['Weight']:.4f}, \"\n",
    "            f\"{row['AllocationTag']}, \"\n",
    "            f\"expected_return={row['ExpectedReturn']:.4f}, \"\n",
    "            f\"volatility={row['Volatility']:.4f}, \"\n",
    "            f\"variance_contri={row['VarianceContribution']:.2f}%, \"\n",
    "            f\"Sharpe_contri={row['SharpeContribution']:.4f}\"\n",
    "        )\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_csv = os.path.join(OUTPUT_DIR, \"final_nonzero_holdings.csv\")\n",
    "    nonzero_df.to_csv(final_csv, index=False)\n",
    "    print(f\"\\nSaved extended holdings summary to: {final_csv}\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "    return {\n",
    "        'prices': prices,\n",
    "        'mu': mu,\n",
    "        'Sigma': Sigma,\n",
    "        'ef': ef_df,\n",
    "        'weights': weights_df,\n",
    "        'best': {\n",
    "            'ret': best_ret,\n",
    "            'risk': best_risk,\n",
    "            'sharpe': best_sharpe,\n",
    "            'weights': weights_series\n",
    "        }\n",
    "    }\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
